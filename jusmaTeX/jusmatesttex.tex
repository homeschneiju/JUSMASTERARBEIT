\documentclass[a4paper, 11pt]{article}
 \usepackage{natbib}
 \usepackage{pdfpages}
\usepackage{graphicx}
 \usepackage[normalem]{ulem}
 \usepackage[T1]{fontenc}
%\useunder{\uline}{\ul}{}
\usepackage{amsmath}

\title{Comparison of ARIMA, SVR and ANN models including exogenous variables for short-term freight demand forecasting at a large national food distributor}
\author{Juliana Schneider}

\begin{document}
	
\maketitle
\section{Introduction}


While technological progress in the past decades has led to a significant decrease in relative emissions per tonne-kilometre in truck haulage, this achievement is compensated by an increase in freight transportation \citep{Umweltbundesamt.2018}. 
In fact, absolute emissions of carbon dioxide rose by 20 \% between 1995 and 2017, while freight transportation in general increased by 64\% between 1991 and 2016 \citep{.uba.2018}\\
Between 2010 and 2030, according to \cite{.bmvi}, transport volumes for road haulage (in millions of tons) will face an estimated increase of 17\% while its transport performance (in billion kilometres) will increase by about 39\%. Thus, about 80\% and 74\% of predicted growth until 2030 for transport volumes and performance respectively will be due to road haulage.\\
\newline

Traditionally, freight transportation demand models are concerned with either vehicle or commodity movements, focusing on origin-destination matrices and geographical scope (e.g. urban, regional, international \citep{Regan.2001}. 
% these global models seem to be rather fit for political decision makers than for individual companies.\
Accordingly, the question of how many passengers and how much of different kinds of commodities is transported by which type of vehicle on which route is a task of interest for political institutions such as the German Federal Ministry of Transport and Infrastructure (Bundesministerium f√ºr Verkehr und Infrastruktur, BMVI) as it is essential for the planning of infrastructure based on transportation demand.\citep{.bmvi}\\
Furthermore, in order to comp with the enormous impact of road haulage on climate, it is a major task for the government to improve and adapt infrastructures as well as strategic planning to transportation demands \citep{BundesministeriumfurUmweltNaturschutzundnukleareSicherheit.2018}.\\

However, considering the growing demand for freight transportation, not only politics but also companies share an interest in the modeling of freight transportation demand \citep{Tsekeris.2011}.\\

What's more, according to \cite{Fite.2002}, freight forecasting can help improve profitability as empty or imbalanced trips are reduced. Furthermore, by planning efficiently, the drivers' quality of life and work-life-balance is enhanced and employee turnover is reduced. This, of course, again reduces a company's expenses, since a high turnover rate imposes costs by the need to constantly hire and train new employees.\\
Another advantage of sophisticated freight demand forecasting is the enhancement of customer satisfaction through shorter delivery times, higher reliability and adapted pricing.\\

Unfortunately, little advancement has been made in the forecasting of short-term freight demand for truckage companies so far. This thesis now aims at building a bridge towards the objective of providing a useful,  and applicable model to users.\\
By comparing three different kinds of methods - a rather classic time series approach and two machine learning models - combined with the incorporation of exogenous variables provided by public sources, first advice for future forecasts for this business case shall be given as well as a framework for further research.

After a brief description of the use case at hand, an overview of existing approaches, the theoretical background for variable selection and statistical methods used in this thesis is provided, followed by an introduction to the business case, the truckload company providing the data and a description of these data. Afterwards, the procedure of variable selection, model establishment and model testing is decribed in detail. Finally, the results achieved for this business case are reported and discussed. 

\section{Use Case: Short-term forecasting of freight weight for road haulage - on the example of NAGEL group}
\subsection{NAGEL group}
\subsection{The data}
\subsection{External data}
hier nur kurz beschreiben, dass es externe Daten geben wird.

\section{State of th Art in the Modeling of Freight Transportation Demand}

NOCH USE OF EXTERNAL DATA BESCHREIBEN

First of all, while browsing literature on the topic of freight transport demand, the sheer amount of different aspects to take into account and consequently the various different quantities under observation in different papers was striking. Common measures in freight demand modeling are: tonne-kilometres, mode choice, freight volume, origin-destination-matrices, and so on.\\
As the goal of this thesis is to establish a model to predict short-term freight demand for a single user, comparability and therefore scalability of the target variable is not necessary. Hence, the target variable "weight" included in the data provided by NAGEL is not altered, but of course, in further works it might be of use to modify the target variable, e.g. to value/weight-ratios.
\\
\\
As initially mentioned, freight transport modeling has so far been concerned with the so-called four-step transport models consisting of trip generation (number of trips from or to origin), trip distribution (destination of trips from origin), mode choice, and trip assignment (choice of path or route between origin and desitnation), adopted from traditional passenger transport modeling\citep{JONG.2004, TransportandInfrastructureCouncil.2016}.
\\
However, as there are differences in freight demand and passenger demand modelling, adaptations have been proposed by \cite{JONG.2004}.
Among other things, the first step in the four-step model - trip generation - should not count the number of trips starting from a certain region of origin, but count the amount of freight (in tonnes) delivered from this origin.\\
In order to construct an entire freight demand model system, the separate measures of all four steps need to be connected, e.g. through value/weight ratios.\\
However, the objective of this thesis is not to establish an entire freight model system but it is rather concerned with questions that would belong to step one.\\
Despite the adaptations proposed by \cite{JONG.2004}, not many other advances have been made. In fact, while passenger transport is a largely studied field due to government regulations, this does not hold true for freight demand \citep{Regan.2001}.\\
\cite{JONG.2004} further states that within step one, models so far have only used aggregate data, where time series analysis models, with and without exogenous variables, have already been applied to short-term forecasting.\\
\cite{Regan.2001}, besides distinguishing between aggregate and disaggregate models, classify into international, intercity and urban freight transportation. Here, at first glance, the category of an intercity model seems suitable, but in this category, freight transportation demand is either modeled as a utility maximisation task or an inventory based model for mode choice and production decision, which again is not the scope of this work.
\\
\\
Meanwhile,  since, as mentioned above, most literature so far is concerned with aggregate data, within the use case at hand the focus is on the analysis of disaggregate data. 
Apart from this, further parts of the four step model are not of interest, i.e. neither other parts of step one (e.g. I/O-models) nor other steps, which is why the research on existing literature was expanded to include more specific studies. 

\cite{Taghizadeh.2017} used artificial neural networks to predict the demand for weather-sensitive retail products. \\
\cite{Intihar.2017} predicted container throughput using Dynamic Factor Anaysis and ARIMAX models.\\
\cite{Khandelwal.2015} established hybrid ARIMA and ANN models based on DWT decomposition for general time series analysis.
\cite{Miller.2018} used ARIMA to predict full truckload transportation prices.\\
\cite{Mircetic.2016} applied SARIMA models to forecast the demand in a beverage supply chain.\\
\cite{Gao.2011} combined grey models, ANN and Support Vector Machines to model full truckload volume.
\cite{Andreoni.2006b} use both univariate and multivariate ARIMA models, examining the impact of new links, new destinations and lower fares on air transport demand at the Port of Reggio Calabria.
\cite{JoaoF.L.Oliveira.2014} build a hybrid of ARIMA and SVR for general long term time series prediction.
\cite{Kourentzes.2014} suggest Ensemble Neural Network models to yield more accurate results in time series forecasting.

As is evident, all of the papers mentioned above applied models to either measures similar to but not exactly weight or time series analysis in general. Several authors proposed hybrid models consisting of combined versions of the basic models to be compared further below. \\
Regardless of the lack of literature exactly matching the target of this thesis, the combined information gained from all of these papers provides reasonable background to be supportive of it. Furthermore, this scarcity in literature proves the necessity of a primary approach to adress the objective of the use case at hand.\\
Hence, it is important to first retrace the reasoning behind the choice of rather basic versions of AR(I)MAX, SVR and ANN models for the prediction of freight weight demand, which the following section is dedicated to.\\

\section{Time series}
Forecasting of time series with (S)AR(I)MA models is a well-established concept that has been studied thoroughly for many decades and provides good forecasting accuracy \citep{Arlt.2017, Khandelwal.2015}.It has found application in many domains such as economy (MEHR QUELLEN), 

The assumption ARIMA models are based on states that the values of a target variable are generated by a linear combination of past values of the same variable and white noise \citep{Khandelwal.2015}, thus making it a stochastic process, "i.e. an ordered sequence of random variables" \citep{Andreoni.2006b}, with data entries at equally distant intervals \citep{Hunt.2003}.\\
A mathematical assumption underlying times series processes is stationarity.
(Weak) Stationarity is given when mean and covariance are independent oft time \textit{t}, and the relationship between two values at time points \textit{t} and \textit{t + i} is the same as the relationship between two values at time points \textit{s} and \textit{s + i}, i.e. independent of the exact position in the time series, but provided the distance between any two values is the same \citep{Vogel.2015}.\\
If $\mu(t) \neq \mu$, i.e. the mean is not independent of time, the assumption of stationarity is violated; there is a so-called trend. In the case of a non-stationary time series, stationarity can be obtained by differencing \citep{Andreoni.2006b, Hunt.2003}
First-order differencing is denoted as:
\begin{equation}
	BY_{t} = Y_{t} - Y_{t-1}
\end{equation}
, where B is the backshift operator \citep{Hunt.2003}.
If $\sigma^2(t) \neq \sigma$, i.e. the variance is not independent of time, the time series can be logarithmised to obtain stationarity \citep{Andreoni.2006b}.
Tests for stationarity include (Augmented) Dickey-Fuller-Test (ADF test) \citep{Zhao.07022018} and a visual check of the time series plot. 

ADF tests are part of the family of unit roots tests, i.e. the null hypothesis states that the process is non-stationary \citep{Zhao.07022018}. See \cite{Vogel.2015} for a detailed description of the test metric. (ODER DOCH AUSF√úHRLICHER?)

Another condition of time series processes is \textbf{causality}, i.e. future values only depend on current and past values and not on future values themselves \citep{Vogel.2015}.

If there are recurrent fluctuations that ocurr yearly or otherwise periodically, there is a seasonal or cyclical component, respectively \citep{Vogel.2015} that has to be taken into account; the time series has to be "deseasonalized" accordingly \citep{Benkachcha.2015}.\\ The length of the period of a cyclical component may be determined through a fourier series \citep{Vogel.2015}.
Tests for seasonal components include ??? and visual checks of the time series plots.

\subsection{AR models}
Autoregressive (AR) processes are processes where a value of a variable at \textit{t} depends on weighted previous values of the variable itself plus a white noise term $e \sim  WN(0, \sigma_{\epsilon}^2)$. An \textit{AR(p)} process of order \textit{p} has the form:
\begin{equation}
	Y_{t} = C + \phi_{1} y_{t-1} + \phi_{2} y_{t-2} + ... + \phi_{p} y_{t-p} +e_{t}
\end{equation}
where $C$ is the intercept.\\
The order \textit{p} of an AR process may be determined by a visual check of the plotted Partial Autocorrelation function (PACF). 
The PACF and the corresponding empirical PACF are denoted in the following form:
Considering a time series as a regression where $\tau$ denotes the lag:
\begin{equation}
Y_{t+1} = \Phi_{\tau,1}Y_{t+\tau-1} + \Phi_{\tau,2}Y_{t+\tau-2} + ... + \Phi_{\tau,\tau - 1}Y_{t+1} + \Phi_{\tau,\tau}Y_{t} + \epsilon_{t+\tau} √ü \text{for $\tau = 1, 2, ...$}  
\end{equation}
The PACF is now defined as the regression coefficient $\Phi_{\tau,\tau}$.
\begin{equation}
PACF: \pi(\tau):= \begin{cases}
\Phi_{\tau,\tau} & \tau = 1, 2, ... \\
1 & \tau = 0 \\
\pi(-\tau) & \tau = -1, -2, ...
\end{cases}
\end{equation}
with 
$-1 < \pi(\tau) < 1$ \citep{Vogel.2015}. The empirical PACF can be determined by using Yule-Walker equations or the Durbin-Levinson algorithm (for a detailed description see \citep{Vogel.2015}).

\subsection{MA models}
Moving Average (MA) processes of order \textit{q} are denoted like this:
\begin{equation}
	Y_{t} = \mu_{MA} + \epsilon_{t} - \phi_{1}\epsilon_{t-1} - \phi_{2}\epsilon_{t-2} - ... - +\phi_{q}\epsilon_{t-q}
\end{equation}
This means the value of a target variable at \textit{t} depends on a white noise process and previous white noise weighted by $\phi$. $\epsilon$ in this context are called \textit{innovations}. Furthermore, $\mu$ may be 0 and MA-processes are stationary and causal \citep{Vogel.2015}.
The order \textit{q} of an MA process may, equal to AR processes, be determined by a visual check of the Autocorrelation Function (ACF)'s plot (correlogram). The ACF and the empirical ACF are denoted as:
\begin{equation}
ACF:	\rho(\tau) := \frac{\gamma(\tau)}{\gamma(0)} = \frac{Cov(Y_{t},Y_{t+\tau})}{Var(Y_{t})}, \tau \end{equation}

\begin{equation}
empirical ACF: \hat{\rho}_(\tau) := \frac{\hat{\gamma}(\tau)}{\hat{\gamma}(0)} = \frac{\sum_{t = 1}^{n - \tau}(y_{t} - \bar{y})(y_{t + \tau}-\bar{y})}{\sum_{t = 1}^{n}(y_{t}-\bar{y})^2}
\end{equation}

A condition introduced by \cite{Box.1976} is the \textbf{invertibility} of the MA process into an AR($\infty$) process, which ensures that only one parametrization of an MA process can be identified for any ACF.

However, the process underlying time series data may change over time - it is subject to uncertainty \citep{Adhikari.2015}. A time series model may be biased or overfitted as well as its parameters misspecified. \\

\subsection{AR(I)MA models}

As the name suggests, AR(I)MA(p,d,q) - auto-regressive integrated moving average - models model time series data with an AR and an MA component, and trend in data through differencing (which is the "I-part"). \\
The parameters \textit{p}, \textit{d}, and \textit{q} respectively denote the order of the AR component, the degree of differencing and the MA component \citep{Zhao.07022018}.
If there is also a seasonal component - thus a SARIMA model is to be fitted - there are additional parameters P, D, Q referring to the seasonal orders (or degrees) of AR, differencing and MA.\\

\begin{equation}
Y_{t} = + \mu_{ARMA} \sum_{i = 1}^{p}\Phi_{i}Y_{t-i} + e_{t}  + \epsilon_{t} - \sum_{m = 1}^{q} \Theta_{m}\epsilon_{t-m} 
\end{equation}
where $\mu_{ARMA}$ actually equals the intercept $C$ of the aforementioned $AR(p)$ model.

\subsection{The Box-Jenkins program}
In order to iteratively model time series data, \citep{Box.1976} proposed a method to identify suitable parameters - AR, MA and differencing - of an ARIMA model. It consists of the following four steps, as described in \cite{Falk.2012}: \\
\subsubsection{Order selection}
The orders \textit{p} and \textit{q} of the AR and MA have to be determined. This can be done via a visual identification through ACF for MA- and PACF for AR-orders, as mentioned above. \cite{Andreoni.2006b} provide a guideline for visual plot checks. \\
\cite{Vogel.2015} provides an overview for signs of AR, MA and ARMA processes in ACF and PACF plots: \\
\\
\textit{AR(p)} processes:\\
- ACF fades with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\
- PACF breaks off with lag $\tau > p$.\\
\\
\textit{MA(q)} processes:\\
- ACF breaks off with lag $\tau > q$.\\
- PACF fades with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\
\\
\textit{ARMA(p,q)} processes: \\
- Both ACF and PACF fade with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\

For ARMA models, simple plot checks are insufficient and commonly, candidates for \textit{p} and \textit{q} are chosen by minimizing an Information Criterion such as AIC or BIC \citep{Falk.2012}. \\

\subsubsection{Estimation of parameters}
Next, the parameters (or coefficients) of the AR and/or MA components of the model are estimated, e.g. by Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE) \citep{Andreoni.2006b}.  \\
%As mentioned above, the parameters of an AR-process can be estimated using Yule-Walker equations or the OLS-estimator; note that the latter may not result in describing causal process, but both can be tested for significance \citep{Vogel}.\\
%Analogous to the Yule-Walker equations for AR processes, the innovations algorithm may be applied to estimate the parameters of an MA process. 
To estimate the parameters of an ARMA process, MLE is the most common method \citep{Vogel.2015}. However, it assumes a distribution for $Y_{t}$ - usually a normal distribution.
\\
\cite{Vogel.2015} prefers to switch steps 1 and 2, reasoning that the estimation of the parameters depends largely on the order of the ARMA model. However, admittedly to estimate the parameters, the order of the process have to be known beforehand; although they can be re-adapted later on.
For this reason, in this coursework, the procedure according to \cite{Box.1976} will be followed.\\
\\

\subsubsection{Model diagnostics}
These are mainly residual checks \citep{Arlt.2017}. After having fitted the ARIMA-moddels, it is necessary to test the residuals for autocorrelation and normal distribution. Using the Ljung-Box-Test (LB), it is possible to check whether the residuals are autocorrelated, and with the Jarque-Bera-Test (JB), one can check for deviation of the time series from a normal distribution  \citep{Andreoni.2006b}. The null hypothesis in the LB-test is that the time series process $Y_{t}$ consists of independent and identically distributed (\textit{i.i.d.}) random variablest, whereas the null hypothesis in the JB-test states that the random variables $Y_{t}$ are normally distributed. If the conditions do not hold, the model has to be re-specified as in step 1 \citep{Vogel.2015}. \\
\begin{equation}
	LB = N *(N + 2) * \sum_{\tau = 1}^{m}\frac{(\hat{\rho}(\tau))^2}{N - \tau}
\end{equation}
\begin{equation}
	JB = \frac{N - n_{p}}{6}*(S^2 + \frac{(K - 3)^2}{4})
\end{equation}
$N$ is the sample size, $\tau$ the order of the lag, $m$ the number of lags considered, $\rho(\tau)$ the autocorrelation function at lag $\tau$, $n_{p}$ the number of parameters, $S$ the skewness and $K$ the curtosis of the time series. If both tests are statistically significant, then the assumption holds that the error term is a white noise process $e \sim  WN(0, \sigma_{\epsilon}^2)$ \citep{Intihar.2017}.\\

\subsubsection{Forecasting}
The objective in forecasting is to reduce the expected deviation of the estimated outcome $\hat{Y}_{t}$ from the actual outcome $Y_{t}$ of a time series \citep{Falk.2012}:
\begin{equation}
	E(Y_{t} - \hat{Y}_{t})
\end{equation}

To forecast an \textit{AR(p)} model, for $t = 1$, i.e. the first point in time ahead that we have no observed values for, one can simply replace the parameters in equation NUMBER by the parameters obtained in step 2 and insert past values for $Y_{t-h}, \text{h = 1, 2, ...}$. For an $AR(2)$ process, the one step ahead forecast looks like this:
\begin{equation}
	\hat{Y}_{t+1} = C + \Phi_{1}Y_{t} + \Phi_{2}Y_{t-1}
\end{equation}
In \textit{MA(q)} models, reconsider that the innovations are caused by white noise, and for a forecast of $Y_{t+1}$, $\epsilon_{t+1}$ is directly part of the equation whose expected value is 0.  $\epsilon_{t-1}$, so the one step ahead forecast for an $MA(2)$ process looks like this:
\begin{equation}
\hat{Y}_{t+1} = \mu_{MA}  - \Theta_{1}\epsilon_{t}  - \Theta_{2}\epsilon_{t-1}
\end{equation}
But already with a two step ahead forecast, it is obvious that the $MA(2)$ process (as all $MA(q)$ processes) converges towards the mean $\mu$ the farther ahead the forecasting step:
\begin{equation}
\hat{Y}_{t+2} = \mu_{MA}  - \Theta_{1}\hat{\epsilon}_{t+1}  - \Theta_{2}\epsilon_{t} = \mu_{MA} - \Theta_{1} * 0 - \Theta_{2}\epsilon_{t}
\end{equation}
because $E(\epsilon_{t+1}) = 0$.
In $ARMA(p.q)$ models, the farther ahead the forecast, the more $Y_{t+\tau}$ converges towards the expected value of the time series $E(Y_{t+\tau}) = \mu_{ARMA} \ for \ \tau = 0, 1, ...$.\\

\subsection{AR(I)MAX}
 
So far, only univariate time series processes, where $Y_{t}$ is solely predicted by its own past values, $Y_{t-\tau}, \tau = \text{ 1,2,...}$, have been considered. Naturally, the question arises whether it is possible to improve predictions based on further explanatory variables (denoted by $X$ or $X_{t}$ in the following). 
%\begin{equation}
%	Y_{t} = \Phi_{\tau,1}Y_{t+\tau-1} + \Phi_{\tau,2}Y_{t+\tau-2} + ... + \Phi_{\tau,\tau - 1}Y_{t+1} + \Phi_{\tau,\tau}Y_{t} + \epsilon_{t+\tau} + \mu + \epsilon_{t} - \phi_{1}\epsilon_{t-1} - \phi_{2}\epsilon_{t-2} - ... - +\phi_{q}\epsilon_{t-q} + \Delta X_{t-1} \text{for $\tau = 1, 2, ...$} 
%\end{equation}
\\

Unfortunately, literature on AR(I)MAX models is scarce compared to literature on classic univariate time series analysis.\\
\cite{Intihar.2017} use ARIMAX to forecast container throughput with additional information of macro-economic indicators at the Port of Koper, whereas \cite{Durka.2012} forecast macroeconomic time series themselves with both ARIMA and ARIMAX models.\\
\cite{Lee.2010} include Ramadan effect in their prediction of sales data. \\
\cite{Anggraeni.2015} observed differences in the sales of muslim kids' clothing with Eid holidays every year and modeled this with an ARIMAX model. \\
\cite{Cools.2009} compare ARIMAX to SARIMAX modeling in daily traffic counts. \\

Common AR(I)MA models can be extended to so-called ARMAX models by adding lagged explanatory variables to the model \citep{Andreoni.2006b}:

\begin{equation}
Y_{t} = \sum_{i = 1}^{p}\Phi_{i}Y_{t-i} + e{t} + \mu_{ARMA} + \epsilon_{t} - \sum_{m = 1}^{q} \Theta_{m}\epsilon_{t-m} + \gamma X_{t-1} 
\end{equation}

Equation NUMMER can be rewritten in terms of a transfer function model \citep{Durka.2012}:
\begin{equation}
y_{t} = C + v(B)X_{t} + e_{t}
\end{equation}
where $v(B)X_{t}$ is the transfer function with backshift operator \textit{B}:
\begin{equation}
	v(B)X_{t} = \sum_{j = 0}^{\infty} v_{j}B^{j}X_{t}
\end{equation}

AR(I)MAX models can be interpreted as regression problems, where one variable is explained by another; however, in time series, classical linear regression cannot be applied due to autocorrelated error terms \citep{Cools.2009}, such that the standard errors are underestimated QUELLE.


\section{Artificial Neural Networks}


Although seemingly new in the world of statistical tools, Artificial Neural Networks have been invented many decades ago and the enthusiasm for them HAS experienced several revivals throughout the years \citep{Gao.2011}. The idea stems from human brains \citep{Zhang.1998} and the way their neurons learn, store and process information together \citep{Balkin.1997}. Although this sounds like a very complex, ambitious venture, NNs are "simply a parameterized non-linear function that can be fitted to data for prediction purposes" \citep{Nakamura.2005}. The process, although very much data-driven \citep{Zhang.1998} and sensitive to the training data \citep{Adhikari.2015}, it is not non-parametric \citep{Dorffner96neuralnetworks}. \\
can be fitted to data for prediction purposes" 	\citep{Nakamura.2005}. Nowadays, with the advance of fast and high-performance personal computers, basically anyone can fit a neural network to their data \citep{Gao.2011}.\\
A great strength of NNs is their ability to detect structures between variables that have not yet been discovered \citep{Zhang.1998}.\\ 
During these different stages of popularity, ANNs have been developed further. Refined architectures allow the researcher to adapt a neural network to their problem's structure, starting from rather simple one-layer feedforward algorithms to more complex long-short-term-memory multi-layer-perceptron NNs. (QUELLE)\\
Here, a short overview shall be given before discussing the choice of type of NN for this specific case.\\
\\
\subsection{The general structure of NNs}

An NN typically consists of (at least) three layers: One input, one hidden and one output layer. ANNs learn by example \citep{Balkin.1997, Gao.2011}: Neurons in the input-layer receive the input from the data and pass them on to the hidden layer, where information is processed and then passed to the output layer, which usually consists of one single neuron and is equivalent to the target value. (QUELLE)\\

Layers - and neurons - are connected by activation, also called transfer functions, which multiply the output received by the preceding neuron by a weight and pass this new output on to the next layer and so on.\\
Lastly, a learning algorithm is applied to estimate the parameters in the network. When fed some training data, the network's output is compared to the actual output value of the target variable through a loss function. This error is then passed backwards through the layers to adjust the transfer functions' weights.

Multilayer perceptrons are the most popular type of (feedforward) NN \citep{Balkin.1997, Benkachcha.2015}

The most common learning algorithm is backpropagation as proposed by \cite{Rumelhart.1986}; others are e.g. Levenberg-Marquardt or 

\subsection{Network size and structure}

A crucial step in modelling NNs is to define the network size and structure, i.e. define the number of layers and nodes (neurons), in order to accurately depict the complexity of the structure of the data and avoid overfitting or underfitting as well as keep computation times low \citep{Karsoliya.2012}.\\
There is no exact way to determine the number of nodes in each layer, although a lot of previous work has been dedicated to this problem \citep{Karsoliya.2012}. Of course, it depends on the problem at hand.\\
The number of input neurons may be equal to the number of input variables in one's problem, but in a time series, the number of input neurons is not evident \citep{Zhang.1998}.
It is already clear at this point that the number of output neurons is one, as we are interested in one continuous target variable and as is recommended by \cite{Karsoliya.2012}.\\

However, the number of hidden layers and the number of neurons within each hidden layer still has to be determined. \\
As the objective of this thesis is to present basic models to be contrasted with each other, one hidden layer should suffice. Now, the number of hidden neurons has to be established by trial and error - since there is no clear rule - as is done by most researchers \citep{Karsoliya.2012}. If, with an arbitrary small number of hidden neurons, the network should fail to converge even after multiple restarts, the number of hidden neurons should be increased successively. \\

\subsection{Activation functions}
Activation functions - also known as transfer functions - introduce nonlinearity by mapping the input node to the output node\citep{Zhang.1998}. They must be differentiable \citep{Rojas.1996} \\
Generally, the activation $a$ of a neuron $n_{i}$ is given by
 \begin{equation}
a_{i} = f(n_{i})=f(\sum{w_{ij}*x_{j}})
\end{equation} 
where $f(n_{i})$ is the transfer function, $w_{ij}$ is the connection weight between node $j$ and
node i, $x_{j}$ the input signal from the node $j$, and $a_{i}$ is the output of the neuron $i$ \citep{Benkachcha.2015}.
Among the most popular are sigmoid, hyperbolic tangent, sine or cosine, and linear functions.
\begin{equation}f(x) = (1 + exp(-x))-1\end{equation} sigmoid or logistic. Sigmoid functions can take values strictly between 0 and 1 and their derivatives are always positive \citep{Rojas.1996}.
\begin{equation}f(x) = (exp(x) - exp(-x))/(exp(x)) + exp(-x))\end{equation} Hyperbolic tangens (tanh) function can take values between -1 and 1.
\begin{equation}f(x) = sin(x)\end{equation} or \begin{equation}f(x) = cos(x)\end{equation} sine / cosine can take values between -1 and 1.
\begin{equation}f(x) = x\end{equation} linear can take values between $-\inf$ and $\inf$.

\cite{Zhang.1998} point out that for target variables with continuous values, the use of a linear activation function in the output node is advisable.
It must be noted that HIER SEKUND√ÑRQUELLE COTTRELL ET AL IN ZHANG 1998 linear activation functions may not be applied on data with trends; hence differencing may be necessary before feeding the data to the network.\\

Since most papers reviewed for this thesis mention the use of sigmoid or logistic transfer functions in the hidden nodes and linear activation in the output nodes, it seems a reliable modus operandi.

\subsection{Learning algorithms}

NNs may be trained in various ways. Generally, learning algorithms - also called training algorithms - iteratively minimize an error function to adjust the weights on the nodes \citep{Nakamura.2005}. Backpropagation as the most widely used algorithm is based on the gradient-descent method \citep{Rojas.1996}, which is why differentiability of the activation functions and therefore of the error functions as well is necessary. After a random initialization of the network, the initial weights are adjusted according to the gradient of the error function. The Sum of Squared Errors (SSE) might be considered:
\begin{equation} E = \frac{1}{2}*\sum_{i=1}^{p}||\hat{y}_{i} - t_{i}||\end{equation}
The derivatives of $E$, 
\begin{equation}
\delta E = {\frac{\partial E}{\partial w_{1}}\frac{\partial E}{\partial w_{2}},...,\frac{\partial E}{\partial w_{l}}}
\end{equation}
 are minimized and the weights iteratively updated:

\begin{equation}
w_{i} = -\gamma\frac{\partial E}{\partial w_{i}} for \textit{i} = 1, ...., \textit{l}
\end{equation}
, where $\gamma$ is the step size, also called learning rate. It determines the extent to which the weights are changed in each iteration and must carefully be specified, since too steep a descent might not converge and a too shallow one might be too slow \citep{Zhang.1998}.\\
Finally, $\delta E$ is expected to be zero.
When given input data, the network is fed forward by calculating the input to each node,
\begin{equation}
x_j = \sum_{i}y_{i}*w_{ji}
\end{equation}, where \textit{j} indexes output nodes, \textit{i} indexes the nodes connected to node \textit{j} $w_{ji}$ denotes the weight and $y_{i}$ denotes the antecedent's node's output; outputs in turn are calculated by their activation functions. \citep{Rumelhart.1986}
Another type of error function may be Mean Squared Error (MSE) \citep{Zhang.1998}. Note that independently of the training algorithm, the network might get stuck at a local optimum.

The possible flaws of static learning rates have been tried to overcome by expanding the classic gradient descent method with a so-called momentum parameter that controls the direction of the change in weights to not diverge too far from the direction of the antecedent change in weight. (QUELLE)

\subsection{An overview over types of Artificial Neural Networks} 

\cite{Dorffner96neuralnetworks} counts feedforward ANNs, Jordan ANNs, Elman ANNs (the latter two being types of RNNs); and Multicurrent Networks as most relevant for time series analysis.
According to \cite{Balkin.1997}, RNNs are better suited for time series data than FNNS; in fact, bad results in an RNN may hint that there is not time dependency in the data at all. However, a feedforward NN in its simplest form reduces to an $AR[p]$-model whereas an RNN may reduce to an $ARMA$-model.\\

Due to their ability to approximate any (linear or nonlinear) function underlying the data arbitrarily closely by a nonlinear function, MLPs and RBFNNs classify as universal function approximators \citep{Dorffner96neuralnetworks}. They can reduce to an AR[p]-model with an appropriate function that is estimated with a learning algorithm.\\
Considering the vast number of different types of NNs as well as their general flexibility, it is not a trivial task to choose a suitable model, specify it, and train it \citep{Kourentzes.2014}. 
\section{Model Choice}

There are several ways to model time series data, some of which have been well established for many decades already, while others have recently emerged in the space of popular methods as well . ARIMA models are an example of the former kind of methods, while SVM and ANN are examples of the latter kind. \citep{Adhikari.2015}\\
\cite{JoaoF.L.Oliveira.2014} state ARIMA, SVM and ANN as the three major methods used in time series analysis. \\

ANNs are particularly useful as a data-driven method, if there are data but difficulties in describing the process behind them \citep{Zhang.1998}. In the case at hand, as there ist not much literature exactly fitting the purpose of my research question, ANNs might be a convenient approach to handle the data.\\

All three of the aforementioned methods may prove suitable to forecast freight weight, as they each have advantages. Before going into detail about each method and specifying each model according to the data, in this section, ARIMA, SVR and ANN shall be directly contrasted in order to give a brief justification as to why these three were selected to be compared in this thesis.

Later, a conceptual choice of the specific models for each method (e.g. choice of kernel, type of NN) shall be made before specifying them exactly according to the data in the application.

%\begin{table}[]
%	\resizebox{\textwidth}{!}{
%		\begin{tabular}{llll}
%			& \textbf{ARIMA} & \textbf{SVR} & \textbf{ANN} \\
%			\textbf{Pro} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Efficient (Adhikari 2014)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Models linear\\   as well as nonlinear relationships (Han 2014,\\   Adhikari 2014)\\   ¬∑       \\   Only few data required (Wu 2008)\\   ¬∑       \\   Variable selection\\   ¬∑       \\   Able to find global maximum (Han 2014)\\   ¬∑       \\   strong generalization ability  (Adhikari 2014) \\   ¬∑       \\   faster convergence speed (Wu 2008)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   models nonlinear relationships\\   ¬∑       \\   variable selection\\   ¬∑       \\   useful for ‚Äûlong-term‚Äú time series \\   (Han 2014)\\   ¬∑       \\   non-parametric and nonlinear modeling skill, flexible and data-driven nature,\\   good generalizationability, and reasonably good accuracy. (Adhikari 2014)\\   ¬∑       \\   RNN capable of including time dependencies (Adhikari 2014)\end{tabular} \\
%			\textbf{Con} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Models linear relationships between variables ‚Äì accuracy my be\\   afflicted (Adhikari 2014)\\   ¬∑       \\   Easy to be disturbed by noise (Wu 2008)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Training times increase\\   drastically with respect to the number of training samples (Deng 2005)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   May lead to overfitting (Deng 2005)\\   ¬∑       \\   May only find local maximum (Deng 2005, Adhikari 2014)\\   ¬∑       \\   Sensitive for preselected model architecture and data fed to the model\\   (Adhikari 2014)\\   ¬∑       \\   Computational cost RNN \textgreater FNN (Adhikari 2014)\end{tabular} \\
%			&  &  & 
%		\end{tabular}
%	}
%\end{table}

\section{Variable selection}

A well-defined model should include all relevant explanatory variables that contribute to the variance found in the target value and exclude irrelevant, noisy variables. This way, the data and the process constituting it can be better understood, computational time and the so-called curse of dimensionality can be reduced while maintaining a high prediction accuracy \citep{Chandrashekar.2014}. The curse of dimensionality describes the problem arising in a high dimensional (i.e. containing many variables) space, where one needs very large samples to accurately depict said space (QUELLE). Variable selection is furthermore important as irrelevant data may cause poor generalization when given new data \citep{Chandrashekar.2014}.
\\
A first selection of variables should be given by experts. (QUELLE)
Next to the researchers' expertise, there exist several variable selection (also called feature selection) methods that quantify the explanatory value of each possible variable and include and / or exclude them in the final model by a prespecified selection criterion.
Directly comparing all $2^N$ possible subsets of variables is computationally unfeasible, which is why usually metaheuristics are applied. The most common types of metaheuristic variable selection methods shall be described in the following, as well as the choice process for a variable selection method for this case.
\\
In their study on the influence of national, regional and local economic indices on freight volume for different segments of the truckload industry in the USA, \cite{Fite.2002} first examined correlation matrices before using stepwise multiple regression to draw a selection from the large set of initial variables collected. 
\cite{Taghizadeh.2017} suggest using ensemble methods such as Random Forest to take advantage of several different machine learning methods.
\cite{Feng.2018} used Spearman rank correlation to select external variables to predict railway freight volume.


Albeit the growing number of available methods for variable selection, neither is there one gold standard that is generally useful, nor is there one method especially advisable for each kind of data \citep{Taghizadeh.2017}\\

Generally, variable selection methods, also called feature selection methods in the context of Machine Learning, can be classified into \textit{Filter}, \textit{Wrapper} and \textit{Embedded methods}. 


\bibliographystyle{apalike}
\bibliography{jusmabib}
\end{document}