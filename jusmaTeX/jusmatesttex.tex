\documentclass[a4paper, 11pt]{article}
 \usepackage{natbib}
 \usepackage{pdfpages}
\usepackage{graphicx}
 \usepackage[normalem]{ulem}
 \usepackage[T1]{fontenc}
%\useunder{\uline}{\ul}{}
\usepackage{amsmath}

\title{Comparison of ARIMA, SVR and ANN models including exogenous variables for short-term freight demand forecasting at a large national food distributor}
\author{Juliana Schneider}

\begin{document}
	
\maketitle
\section{Introduction}


While technological progress in the past decades has led to a significant decrease in relative emissions per tonne-kilometre in truck haulage, this achievement is compensated by an increase in freight transportation \citep{Umweltbundesamt.2018}. 
In fact, absolute emissions of carbon dioxide rose by 20 \% between 1995 and 2017, while freight transportation in general increased by 64\% between 1991 and 2016 \citep{.uba.2018}\\
Between 2010 and 2030, according to \cite{.bmvi}, transport volumes for road haulage (in millions of tons) will face an estimated increase of 17\% while its transport performance (in billion kilometres) will increase by about 39\%. Thus, about 80\% and 74\% of predicted growth until 2030 for transport volumes and performance respectively will be due to road haulage.\\
\newline

Traditionally, freight transportation demand models are concerned with either vehicle or commodity movements, focusing on origin-destination matrices and geographical scope (e.g. urban, regional, international \citep{Regan.2001}. 
% these global models seem to be rather fit for political decision makers than for individual companies.\
Accordingly, the question of how many passengers and how much of different kinds of commodities is transported by which type of vehicle on which route is a task of interest for political institutions such as the German Federal Ministry of Transport and Infrastructure (Bundesministerium f√ºr Verkehr und Infrastruktur, BMVI) as it is essential for the planning of infrastructure based on transportation demand.\citep{.bmvi}\\
Furthermore, in order to comp with the enormous impact of road haulage on climate, it is a major task for the government to improve and adapt infrastructures as well as strategic planning to transportation demands \citep{BundesministeriumfurUmweltNaturschutzundnukleareSicherheit.2018}.\\

However, considering the growing demand for freight transportation, not only politics but also companies share an interest in the modeling of freight transportation demand \citep{Tsekeris.2011}.\\

What's more, according to \cite{Fite.2002}, freight forecasting can help improve profitability as empty or imbalanced trips are reduced. Furthermore, by planning efficiently, the drivers' quality of life and work-life-balance is enhanced and employee turnover is reduced. This, of course, again reduces a company's expenses, since a high turnover rate imposes costs by the need to constantly hire and train new employees.\\
Another advantage of sophisticated freight demand forecasting is the enhancement of customer satisfaction through shorter delivery times, higher reliability and adapted pricing.\\

Unfortunately, little advancement has been made in the forecasting of short-term freight demand for truckage companies so far. This thesis now aims at building a bridge towards the objective of providing a useful,  and applicable model to users.\\
By comparing three different kinds of methods - a rather classic time series approach and two machine learning models - combined with the incorporation of exogenous variables provided by public sources, first advice for future forecasts for this business case shall be given as well as a framework for further research.

After a brief description of the use case at hand, an overview of existing approaches, the theoretical background for variable selection and statistical methods used in this thesis is provided, followed by an introduction to the business case, the truckload company providing the data and a description of these data. Afterwards, the procedure of variable selection, model establishment and model testing is decribed in detail. Finally, the results achieved for this business case are reported and discussed. 

\section{Use Case: Short-term forecasting of freight weight for road haulage - on the example of NAGEL group}
\subsection{NAGEL group}
\subsection{The data}
\subsection{External data}
hier nur kurz beschreiben, dass es externe Daten geben wird.

\section{State of th Art in the Modeling of Freight Transportation Demand}

NOCH USE OF EXTERNAL DATA BESCHREIBEN

First of all, while browsing literature on the topic of freight transport demand, the sheer amount of different aspects to take into account and consequently the various different quantities under observation in different papers was striking. Common measures in freight demand modeling are: tonne-kilometres, mode choice, freight volume, origin-destination-matrices, and so on.\\
As the goal of this thesis is to establish a model to predict short-term freight demand for a single user, comparability and therefore scalability of the target variable is not necessary. Hence, the target variable "weight" included in the data provided by NAGEL is not altered, but of course, in further works it might be of use to modify the target variable, e.g. to value/weight-ratios.
\\
\\
As initially mentioned, freight transport modeling has so far been concerned with the so-called four-step transport models consisting of trip generation (number of trips from or to origin), trip distribution (destination of trips from origin), mode choice, and trip assignment (choice of path or route between origin and desitnation), adopted from traditional passenger transport modeling\citep{JONG.2004, TransportandInfrastructureCouncil.2016}.
\\
However, as there are differences in freight demand and passenger demand modelling, adaptations have been proposed by \cite{JONG.2004}.
Among other things, the first step in the four-step model - trip generation - should not count the number of trips starting from a certain region of origin, but count the amount of freight (in tonnes) delivered from this origin.\\
In order to construct an entire freight demand model system, the separate measures of all four steps need to be connected, e.g. through value/weight ratios.\\
However, the objective of this thesis is not to establish an entire freight model system but it is rather concerned with questions that would belong to step one.\\
Despite the adaptations proposed by \cite{JONG.2004}, not many other advances have been made. In fact, while passenger transport is a largely studied field due to government regulations, this does not hold true for freight demand \citep{Regan.2001}.\\
\cite{JONG.2004} further states that within step one, models so far have only used aggregate data, where time series analysis models, with and without exogenous variables, have already been applied to short-term forecasting.\\
\cite{Regan.2001}, besides distinguishing between aggregate and disaggregate models, classify into international, intercity and urban freight transportation. Here, at first glance, the category of an intercity model seems suitable, but in this category, freight transportation demand is either modeled as a utility maximisation task or an inventory based model for mode choice and production decision, which again is not the scope of this work.
\\
\\
Meanwhile,  since, as mentioned above, most literature so far is concerned with aggregate data, within the use case at hand the focus is on the analysis of disaggregate data. 
Apart from this, further parts of the four step model are not of interest, i.e. neither other parts of step one (e.g. I/O-models) nor other steps, which is why the research on existing literature was expanded to include more specific studies. 

\cite{Taghizadeh.2017} used artificial neural networks to predict the demand for weather-sensitive retail products. \\
\cite{Intihar.2017} predicted container throughput using Dynamic Factor Anaysis and ARIMAX models.\\
\cite{Khandelwal.2015} established hybrid ARIMA and ANN models based on DWT decomposition for general time series analysis.
\cite{Miller.2018} used ARIMA to predict full truckload transportation prices.\\
\cite{Mircetic.2016} applied SARIMA models to forecast the demand in a beverage supply chain.\\
\cite{Gao.2011} combined grey models, ANN and Support Vector Machines to model full truckload volume.
\cite{Andreoni.2006b} use both univariate and multivariate ARIMA models, examining the impact of new links, new destinations and lower fares on air transport demand at the Port of Reggio Calabria.
\cite{JoaoF.L.Oliveira.2014} build a hybrid of ARIMA and SVR for general long term time series prediction.
\cite{Kourentzes.2014} suggest Ensemble Neural Network models to yield more accurate results in time series forecasting.

As is evident, all of the papers mentioned above applied models to either measures similar to but not exactly weight or time series analysis in general. Several authors proposed hybrid models consisting of combined versions of the basic models to be compared further below. \\
Regardless of the lack of literature exactly matching the target of this thesis, the combined information gained from all of these papers provides reasonable background to be supportive of it. Furthermore, this scarcity in literature proves the necessity of a primary approach to adress the objective of the use case at hand.\\
Hence, it is important to first retrace the reasoning behind the choice of rather basic versions of AR(I)MAX, SVR and ANN models for the prediction of freight weight demand, which the following section is dedicated to.\\



\section{Model Choice and Specification}

There are several ways to model time series data, some of which have been well established for many decades already, while others have recently emerged in the space of popular methods as well . ARIMA models are an example of the former kind of methods, while SVM and ANN are examples of the latter kind. \citep{Adhikari.2015}\\
\cite{JoaoF.L.Oliveira.2014} state ARIMA, SVM and ANN as the three major methods used in time series analysis. \\

ANNs are particularly useful as a data-driven method, if there are data but difficulties in describing the process behind them \citep{Zhang.1998}. In the case at hand, as there ist not much literature exactly fitting the purpose of my research question, ANNs might be a convenient approach to handle the data.\\

All three of the aforementioned methods may prove suitable to forecast freight weight, as they each have advantages (see table NUMMER). Furthermore, it might prove insightful to contrast a classical statistical, a machine learning and a deep learning method with each other.


\subsection{Model specification}

As we shall see, for model specification - i.e. the modelling of the parameters - \textit{Information Criteria} are commonly used. For this reason, the most prevalent of them, the \textit{Akaike Information Criterion} \cite{Akaike.1974} and the \textit{Bayesian Information Criterion} (or \textit{Schwarz Information Criterion}, \cite{Schwarz.1978}) are shortly introduced in this section before moving on to descriptions of the specific types models.
\begin{align}
AIC = 2k - 2ln(\hat{L})\\
BIC = ln(n)k - 2ln(\hat{L})
\end{align}
$\hat{L}$ denotes the Log-Likelihood of the model, $k$ the number of parameters and $n$ the sample size. The \textit{AIC}'s aim is to choose the model that approximates the true data generating process as closely as possible \cite{Wit.2012}. Generally, the model yielding the smallest \textit{AIC} or \textit{BIC} respectively is chosen \cite{Hastie.2017}.
Neither one of the two criteria is superior to the other; \textit{AIC} tends to overfit the larger the sample size, whereas \textit{BIC} tends to choose overly simple models in finite sample sizes \citep{Hastie.2017}.\\
Both of them have been used in the three models presented in this coursework.
In the following sections, each model's theoretical framework and their associated specification techniques shall be illuminated in more detail. First, time series models are introduced as the constitutive baseline of time series modelling. Then, SVR and NNs are presented.

\section{Time series}
Forecasting of time series with (S)AR(I)MA models is a well-established concept that has been studied thoroughly for many decades and provides good forecasting accuracy \citep{Arlt.2017, Khandelwal.2015}.It has found application in many domains such as economy (MEHR QUELLEN), 

The assumption ARIMA models are based on states that the values of a target variable are generated by a linear combination of past values of the same variable and white noise \citep{Khandelwal.2015}, thus making it a stochastic process, "i.e. an ordered sequence of random variables" \citep{Andreoni.2006b}, with data entries at equally distant intervals \citep{Hunt.2003}.\\
A mathematical assumption underlying times series processes is stationarity.
(Weak) Stationarity is given when mean and covariance are independent oft time \textit{t}, and the relationship between two values at time points \textit{t} and \textit{t + i} is the same as the relationship between two values at time points \textit{s} and \textit{s + i}, i.e. independent of the exact position in the time series, but provided the distance between any two values is the same \citep{Vogel.2015}.\\

\subsection{AR models}
Autoregressive (AR) processes are processes where a value of a variable at \textit{t} depends on weighted previous values of the variable itself plus a white noise term $e \sim  WN(0, \sigma_{\epsilon}^2)$. An \textit{AR(p)} process of order \textit{p} has the form:
\begin{equation}
	Y_{t} = C + \phi_{1} y_{t-1} + \phi_{2} y_{t-2} + ... + \phi_{p} y_{t-p} +e_{t}
\end{equation}
where $C$ is the intercept.\\
The order \textit{p} of an AR process may be determined by a visual check of the plotted Partial Autocorrelation function (PACF). 
The PACF and the corresponding empirical PACF are denoted in the following form:
Considering a time series as a regression where $\tau$ denotes the lag:
\begin{equation}
Y_{t+1} = \Phi_{\tau,1}Y_{t+\tau-1} + \Phi_{\tau,2}Y_{t+\tau-2} + ... + \Phi_{\tau,\tau - 1}Y_{t+1} + \Phi_{\tau,\tau}Y_{t} + \epsilon_{t+\tau} √ü \text{for $\tau = 1, 2, ...$}  
\end{equation}
The PACF is now defined as the regression coefficient $\Phi_{\tau,\tau}$.
\begin{equation}
PACF: \pi(\tau):= \begin{cases}
\Phi_{\tau,\tau} & \tau = 1, 2, ... \\
1 & \tau = 0 \\
\pi(-\tau) & \tau = -1, -2, ...
\end{cases}
\end{equation}
with 
$-1 < \pi(\tau) < 1$ \citep{Vogel.2015}. The empirical PACF can be determined by using Yule-Walker equations or the Durbin-Levinson algorithm (for a detailed description see \citep{Vogel.2015}).

\subsection{MA models}
Moving Average (MA) processes of order \textit{q} are denoted like this:
\begin{equation}
	Y_{t} = \mu_{MA} + \epsilon_{t} - \phi_{1}\epsilon_{t-1} - \phi_{2}\epsilon_{t-2} - ... - +\phi_{q}\epsilon_{t-q}
\end{equation}
This means the value of a target variable at \textit{t} depends on a white noise process and previous white noise weighted by $\phi$. $\epsilon$ in this context are called \textit{innovations}. Furthermore, $\mu$ may be 0 and MA-processes are stationary and causal \citep{Vogel.2015}.
The order \textit{q} of an MA process may, equal to AR processes, be determined by a visual check of the Autocorrelation Function (ACF)'s plot (correlogram). The ACF and the empirical ACF are denoted as:
\begin{equation}
ACF:	\rho(\tau) := \frac{\gamma(\tau)}{\gamma(0)} = \frac{Cov(Y_{t},Y_{t+\tau})}{Var(Y_{t})}, \tau \end{equation}

\begin{equation}
empirical \ ACF: \hat{\rho}_(\tau) := \frac{\hat{\gamma}(\tau)}{\hat{\gamma}(0)} = \frac{\sum_{t = 1}^{n - \tau}(y_{t} - \bar{y})(y_{t + \tau}-\bar{y})}{\sum_{t = 1}^{n}(y_{t}-\bar{y})^2}
\end{equation}

A condition introduced by \cite{Box.1976} is the \textbf{invertibility} of the MA process into an AR($\infty$) process, which ensures that only one parametrization of an MA process can be identified for any ACF.

However, the process underlying time series data may change over time - it is subject to uncertainty \citep{Adhikari.2015}. A time series model may be biased or overfitted as well as its parameters misspecified. \\

\subsection{AR(I)MA models}

As the name suggests, AR(I)MA(p,d,q) - auto-regressive integrated moving average - models model time series data with an AR and an MA component, and trend in data through differencing (which is the "I-part"). \\
The parameters \textit{p}, \textit{d}, and \textit{q} respectively denote the order of the AR component, the degree of differencing and the MA component \citep{Zhao.07022018}.
If there is also a seasonal component - thus a SARIMA model is to be fitted - there are additional parameters P, D, Q referring to the seasonal orders (or degrees) of AR, differencing and MA.\\

\begin{equation}
Y_{t} =  \mu_{ARMA} +  \sum_{i = 1}^{p}\Phi_{i}Y_{t-i} + e_{t}  + \epsilon_{t} - \sum_{m = 1}^{q} \Theta_{m}\epsilon_{t-m} 
\end{equation}
where $\mu_{ARMA}$ actually equals the intercept $C$ of the aforementioned $AR(p)$ model.

\subsection{Seasonal AR(I)MA - SARIMA models}

Of course, a time series may also exhibit recurrent seasonal patterns, e.g. an increase in demand in december every year. If there are recurrent fluctuations that ocurr yearly or otherwise periodically, there is a seasonal or cyclical component, respectively \citep{Vogel.2015} that has to be taken into account; the time series has to be "deseasonalized" accordingly \citep{Benkachcha.2015}.\\
The length of the period of a cyclical component may be determined through a fourier series \citep{Vogel.2015}.
seasonal components can be detected by visual checks of the time series plots.
Seasonality is modeled in AR(I)MA models by an additional tuple of orders $(P,D,Q)$; the complete model is then denoted as $SARIMA(p,d,q)(P,D,Q)$, where the additional tuple of orders essentially has the same meaning as the former one, only applied to seasonal magnitude.  
\cite{Hyndman.2008} recommends modeling larger seasonal components (such as yearly reocurrences in daily measured data) or multiple seasonalities by a fourier series.

\subsection{Violation of assumptions and other issues}
If $\mu(t) \neq \mu$, i.e. the mean is not independent of time, the assumption of stationarity is violated; there is a so-called trend. In the case of a non-stationary time series, stationarity can be obtained by differencing \citep{Andreoni.2006b, Hunt.2003}
First-order differencing is denoted as:
\begin{equation}
BY_{t} = Y_{t} - Y_{t-1}
\end{equation}
, where B is the backshift operator \citep{Hunt.2003}.
If $\sigma^2(t) \neq \sigma$, i.e. the variance is not independent of time, the time series can be logarithmised to obtain stationarity \citep{Andreoni.2006b}.
Tests for stationarity include (Augmented) Dickey-Fuller-Test (ADF test) \citep{Zhao.07022018} and a visual check of the time series plot (see Box-Jenkins program further below). 

ADF tests are part of the family of unit roots tests, i.e. the null hypothesis states that the process is non-stationary \citep{Zhao.07022018}. The order \textit{k} that denotes the order of the lag is suggested to be set to $k \approx \sqrt[3]{n-1}$ and reduced successively by t-testing the last parameter $yo$ of equation NUMMER 4 - as long as it does not differ significantly from \textit{zero}, \textit{k} should be further reduced. See \cite{Vogel.2015} for a detailed description of the test metric.

Another condition of time series processes is \textbf{causality}, i.e. future values only depend on current and past values and not on future values themselves \citep{Vogel.2015}.

As with any kind of data, missing values may pose a problem for inference and forecast. While in real applications, the mechanism causing the missingness is unknown, at least for time series, the imputation of values for \textit{Missing At Random (MAR)} and \textit{Missing Completely At Random (MCAR)} mechanisms is nearly identical. \footnote{In MCAR, missingness in the data is completely random, whereas in MAR, missingness in one variable depends on the value of another variable. Lastly, in MNAR, missingness in one variable depends on the value of the variable itself.} Imputation for time series does not solely depend on covariates, but instead on the "hidden variable" time. \cite{Moritz2015ComparisonOD} found that linear interpolation yielded 

\subsection{The Box-Jenkins program}
In order to iteratively model time series data, \citep{Box.1976} proposed a method to identify suitable parameters - AR, MA and differencing - of an ARIMA model. It consists of the following four steps, as described in \cite{Falk.2012}: \\
\subsubsection{Order selection}
The orders \textit{p} and \textit{q} of the AR and MA have to be determined. This can be done via a visual identification through ACF for MA- and PACF for AR-orders, as mentioned above. \\
\cite{Vogel.2015} provides an overview for signs of AR, MA and ARMA processes in ACF and PACF plots: \\
\\
\textit{AR(p)} processes:\\
- ACF fades with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\
- PACF breaks off with lag $\tau > p$.\\
\\
\textit{MA(q)} processes:\\
- ACF breaks off with lag $\tau > q$.\\
- PACF fades with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\
\\
\textit{ARMA(p,q)} processes: \\
- Both ACF and PACF fade with increasing lag $\tau$, possibly sinusoidally or alternatingly.\\


\cite{Zhou.2006} recommend settling on an order of both $p$ and $q$ in a magnitude between 0 and 10, while $d$'s most sensible magnitude is between 0 and 2.\\
For ARMA models, simple plot checks are insufficient and commonly, candidates for \textit{p} and \textit{q} are chosen by minimizing an Information Criterion such as 
the AIC or BIC \citep{Falk.2012}:
\begin{align}
	AIC(p,q) := log(\hat{\sigma}^{2}_{p,q}) + 2 \frac{p + q}{n}\\
	BIC(p,q) := log(\hat{\sigma}^{2}_{p,q}) +  \frac{(p + q)log(n)}{n}\\
\end{align}

\cite{Vogel.2015} check ACF and PACF plots after regular and seasonal differencing to determine der $P$ and $D$ orders in a seasonal ARIMA model. With seasonal ARIMA however, checking ACF and PACF for the regular order $p$ and $q$ is not so easy; it is recommended to initialize the model with high orders for both and then reduce them while checking residuals. 

\subsubsection{Estimation of parameters}
Next, the parameters (or coefficients) of the AR and/or MA components of the model are estimated, e.g. by Ordinary Least Squares (OLS) or Maximum Likelihood Estimation (MLE) \citep{Andreoni.2006b}.  \\
%As mentioned above, the parameters of an AR-process can be estimated using Yule-Walker equations or the OLS-estimator; note that the latter may not result in describing causal process, but both can be tested for significance \citep{Vogel}.\\
%Analogous to the Yule-Walker equations for AR processes, the innovations algorithm may be applied to estimate the parameters of an MA process. 
To estimate the parameters of an ARMA process, MLE is the most common method \citep{Vogel.2015}. However, it assumes a distribution for $Y_{t}$ - usually a normal distribution.
\\
\cite{Vogel.2015} prefers to switch steps 1 and 2, reasoning that the estimation of the parameters depends largely on the order of the ARMA model. However, admittedly to estimate the parameters, the order of the process have to be known beforehand; although they can be re-adapted later on.
For this reason, in this coursework, the procedure according to \cite{Box.1976} will be followed.\\
\\

\subsubsection{Model diagnostics}
These are mainly residual checks \citep{Arlt.2017}. After having fitted the ARIMA-moddels, it is necessary to test the residuals for autocorrelation and normal distribution. Using the Ljung-Box-Test (LB), it is possible to check whether the residuals are autocorrelated, and with the Jarque-Bera-Test (JB), one can check for deviation of the time series from a normal distribution  \citep{Andreoni.2006b}. The null hypothesis in the LB-test is that the time series process $Y_{t}$ consists of independent and identically distributed (\textit{i.i.d.}) random variablest, whereas the null hypothesis in the JB-test states that the random variables $Y_{t}$ are normally distributed. If the conditions do not hold, the model has to be re-specified as in step 1 \citep{Vogel.2015}. \\
\begin{equation}
	LB = N *(N + 2) * \sum_{\tau = 1}^{m}\frac{(\hat{\rho}(\tau))^2}{N - \tau}
\end{equation}
\begin{equation}
	JB = \frac{N - n_{p}}{6}*(S^2 + \frac{(K - 3)^2}{4})
\end{equation}
$N$ is the sample size, $\tau$ the order of the lag, $m$ the number of lags considered, $\rho(\tau)$ the autocorrelation function at lag $\tau$, $n_{p}$ the number of parameters, $S$ the skewness and $K$ the curtosis of the time series. If both tests are statistically significant, then the assumption holds that the error term is a white noise process $e \sim  WN(0, \sigma_{\epsilon}^2)$ \citep{Intihar.2017}.\\

\subsubsection{Forecasting}
The objective in forecasting is to reduce the expected deviation of the estimated outcome $\hat{Y}_{t}$ from the actual outcome $Y_{t}$ of a time series \citep{Falk.2012}:
\begin{equation}
	E(Y_{t} - \hat{Y}_{t})
\end{equation}

To forecast an \textit{AR(p)} model, for $t = 1$, i.e. the first point in time ahead that we have no observed values for, one can simply replace the parameters in equation NUMBER by the parameters obtained in step 2 and insert past values for $Y_{t-h}, \text{h = 1, 2, ...}$. For an $AR(2)$ process, the one step ahead forecast looks like this:
\begin{equation}
	\hat{Y}_{t+1} = C + \Phi_{1}Y_{t} + \Phi_{2}Y_{t-1}
\end{equation}
In \textit{MA(q)} models, reconsider that the innovations are caused by white noise, and for a forecast of $Y_{t+1}$, $\epsilon_{t+1}$ is directly part of the equation whose expected value is 0.  $\epsilon_{t-1}$, so the one step ahead forecast for an $MA(2)$ process looks like this:
\begin{equation}
\hat{Y}_{t+1} = \mu_{MA}  - \Theta_{1}\epsilon_{t}  - \Theta_{2}\epsilon_{t-1}
\end{equation}
But already with a two step ahead forecast, it is obvious that the $MA(2)$ process (as all $MA(q)$ processes) converges towards the mean $\mu$ the farther ahead the forecasting step:
\begin{equation}
\hat{Y}_{t+2} = \mu_{MA}  - \Theta_{1}\hat{\epsilon}_{t+1}  - \Theta_{2}\epsilon_{t} = \mu_{MA} - \Theta_{1} * 0 - \Theta_{2}\epsilon_{t}
\end{equation}
because $E(\epsilon_{t+1}) = 0$.
In $ARMA(p.q)$ models, the farther ahead the forecast, the more $Y_{t+\tau}$ converges towards the expected value of the time series $E(Y_{t+\tau}) = \mu_{ARMA} \ for \ \tau = 0, 1, ...$.\\

\subsection{AR(I)MAX}
 
So far, only univariate time series processes, where $Y_{t}$ is solely predicted by its own past values, $Y_{t-\tau}, \tau = \text{ 1,2,...}$, have been considered. Naturally, the question arises whether it is possible to improve predictions based on further explanatory variables (denoted by $X$ or $X_{t}$ in the following). 
%\begin{equation}
%	Y_{t} = \Phi_{\tau,1}Y_{t+\tau-1} + \Phi_{\tau,2}Y_{t+\tau-2} + ... + \Phi_{\tau,\tau - 1}Y_{t+1} + \Phi_{\tau,\tau}Y_{t} + \epsilon_{t+\tau} + \mu + \epsilon_{t} - \phi_{1}\epsilon_{t-1} - \phi_{2}\epsilon_{t-2} - ... - +\phi_{q}\epsilon_{t-q} + \Delta X_{t-1} \text{for $\tau = 1, 2, ...$} 
%\end{equation}
\\

Unfortunately, literature on AR(I)MAX models is scarce compared to literature on classic univariate time series analysis.\\
\cite{Intihar.2017} use ARIMAX to forecast container throughput with additional information of macro-economic indicators at the Port of Koper, whereas \cite{Durka.2012} forecast macroeconomic time series themselves with both ARIMA and ARIMAX models.\\
\cite{Lee.2010} include Ramadan effect in their prediction of sales data. \\
\cite{Anggraeni.2015} observed differences in the sales of muslim kids' clothing with Eid holidays every year and modeled this with an ARIMAX model. \\
\cite{Cools.2009} compare ARIMAX to SARIMAX modeling in daily traffic counts. \\

Common AR(I)MA models can be extended to so-called ARMAX models by adding lagged explanatory variables to the model \citep{Andreoni.2006b}:

\begin{equation}
Y_{t} = \sum_{i = 1}^{p}\Phi_{i}Y_{t-i} + e{t} + \mu_{ARMA} + \epsilon_{t} - \sum_{m = 1}^{q} \Theta_{m}\epsilon_{t-m} + \gamma X_{t-1} 
\end{equation}

Equation NUMMER can be rewritten in terms of a transfer function model \citep{Durka.2012}:
\begin{equation}
y_{t} = C + v(B)X_{t} + e_{t}
\end{equation}
where $v(B)X_{t}$ is the transfer function with backshift operator \textit{B}:
\begin{equation}
	v(B)X_{t} = \sum_{j = 0}^{\infty} v_{j}B^{j}X_{t}
\end{equation}

AR(I)MAX models can be interpreted as regression problems, where one variable is explained by another so that it can be considered as a linear regression with autocorrelated error terms \citep{Cools.2009}.



\section{Artificial Neural Networks}


Although seemingly new in the world of statistical tools, Artificial Neural Networks have been invented many decades ago and the enthusiasm for them HAS experienced several revivals throughout the years \citep{Gao.2011}. The idea stems from human brains \citep{Zhang.1998} and the way their neurons learn, store and process information together \citep{Balkin.1997}. Although this sounds like a very complex, ambitious venture, NNs are "simply a parameterized non-linear function that can be fitted to data for prediction purposes" \citep{Nakamura.2005}. In principle, an n-dimensional input is mapped onto an m-dimensional output \citep{Rojas.1996}. The process, although very much data-driven \citep{Zhang.1998} and sensitive to the training data \citep{Adhikari.2015}, is not non-parametric \citep{Dorffner96neuralnetworks}. \\
 Nowadays, with the advance of fast and high-performance personal computers, basically anyone can fit a neural network to their data \citep{Gao.2011}.\\
A great strength of NNs is their ability to detect structures between variables that have not yet been discovered \citep{Zhang.1998}.\\ 
During these different stages of popularity, ANNs have been developed further. Refined architectures allow the researcher to adapt a neural network to their problem's structure, starting from rather simple one-layer feedforward algorithms to more complex long-short-term-memory multi-layer-perceptron NNs. (QUELLE)\\
Here, a short overview shall be given before discussing the choice of type of NN for this specific case.\\
\\
\subsection{The general structure of NNs}

An NN typically consists of (at least) three layers: One input, one hidden and one output layer. ANNs learn by example \citep{Balkin.1997, Gao.2011}: Neurons in the input-layer receive the input from the data and pass them on to the hidden layer, where information is processed and then passed to the output layer, which usually consists of one single neuron and is equivalent to the target value. (QUELLE)\\

Layers - and neurons - are connected by activation, also called transfer functions, which multiply the output received by the preceding neuron by a weight and pass this new output on to the next layer and so on.\\
Lastly, a learning algorithm is applied to estimate the parameters in the network. When fed some training data, the network's output is compared to the actual output value of the target variable through a loss function. This error is then passed backwards through the layers to adjust the transfer functions' weights.

Multilayer perceptrons are the most popular type of (feedforward) NN \citep{Balkin.1997, Benkachcha.2015}

The most common learning algorithm is backpropagation as proposed by \cite{Rumelhart.1986}; others are e.g. Levenberg-Marquardt or 

\subsection{Network size and structure}

A crucial step in modelling NNs is to define the network size and structure, i.e. define the number of layers and nodes (neurons), in order to accurately depict the complexity of the structure of the data and avoid overfitting or underfitting as well as keep computation times low \citep{Karsoliya.2012}.\\
There is no exact way to determine the number of nodes in each layer, although a lot of previous work has been dedicated to this problem \citep{Karsoliya.2012}. Of course, it depends on the problem at hand.\\
The number of input neurons may be equal to the number of input variables in one's problem, but in a time series, the number of input neurons is not evident \citep{Zhang.1998}. However, obviously for each exogenous variable added there has to be (at least) one input node added to the NN.\\
It is already clear at this point that the number of output neurons is one, as we are interested in one continuous target variable and as is recommended by \cite{Karsoliya.2012}.\\

However, the number of hidden layers and the number of neurons within each hidden layer still has to be determined. \\
As the objective of this thesis is to present basic models to be contrasted with each other, one hidden layer should suffice. Now, the number of hidden neurons has to be established by trial and error - since there is no clear rule - as is done by most researchers \citep{Karsoliya.2012}. If, with an arbitrary small number of hidden neurons, the network should fail to converge even after multiple restarts, the number of hidden neurons should be increased successively. \\
To determine a suitable number of hidden nodes, \cite{Adhikari.2015} use the BIC:
\begin{equation}
	BIC_{p,h} := h(p+2) + 1
\end{equation}

Due to their ability to approximate any (linear or nonlinear) function underlying the data arbitrarily closely by a nonlinear function, MLPs and RBFNNs classify as universal function approximators \citep{Dorffner96neuralnetworks}. They can reduce to an AR[p]-model with an appropriate function that is estimated with a learning algorithm.\\
Considering the vast number of different types of NNs as well as their general flexibility, it is not a trivial task to choose a suitable model, specify it, and train it \citep{Kourentzes.2014}. 

In time series analysis with NNs, $Y_{t+1}$ is the output that is calculated by feeding the network lagged past obervations of $Y_{t}, \ t for 1,2,... T-1$ and possibly lagged values of exogenous variables \citep{Kourentzes.2014}.
For this purpose, the time series is divided into equally sized chunks and fed to the network, where the the next value in the chunk is expected to be determined by the preceding elements of the same chunk \citep{Pinto}.


\cite{Dorffner96neuralnetworks} counts feedforward ANNs, Jordan ANNs, Elman ANNs (the latter two being types of RNNs), and Multicurrent Networks as most relevant for time series analysis.
Feed-forward NNs an Recurrent NNs shall be described in more detailed in the following subsections.


\subsection{Activation functions}
Activation functions - also known as transfer functions - introduce nonlinearity by mapping the input node to the output node\citep{Zhang.1998}. They must be differentiable \citep{Rojas.1996}. \\
Generally, the activation $a$ of a neuron $n_{i}$ is given by
 \begin{equation}
a_{i} = f(n_{i})=f(\sum{w_{ij}*x_{j}})
\end{equation} 
where $f(n_{i})$ is the transfer function, $w_{ij}$ is the connection weight between node $j$ and
node i, $x_{j}$ the input signal from the node $j$, and $a_{i}$ is the output of the neuron $i$ \citep{Benkachcha.2015}.
Among the most popular are sigmoid, hyperbolic tangent, sine or cosine, and linear functions.
\begin{equation}f(x) = (1 + exp(-x))-1\end{equation} sigmoid or logistic. Sigmoid functions can take values strictly between 0 and 1 and their derivatives are always positive \citep{Rojas.1996}.
\begin{equation}f(x) = (exp(x) - exp(-x))/(exp(x)) + exp(-x))\end{equation} Hyperbolic tangens (tanh) function can take values between -1 and 1.
\begin{equation}f(x) = sin(x)\end{equation} or \begin{equation}f(x) = cos(x)\end{equation} sine / cosine can take values between -1 and 1.
\begin{equation}f(x) = x\end{equation} linear can take values between $-\inf$ and $\inf$.

\cite{Zhang.1998} point out that for target variables with continuous values, the use of a linear activation function in the output node is advisable.
It must be noted that HIER SEKUND√ÑRQUELLE COTTRELL ET AL IN ZHANG 1998 linear activation functions may not be applied on data with trends; hence differencing may be necessary before feeding the data to the network.\\

Since most papers reviewed for this thesis mention the use of sigmoid or logistic transfer functions in the hidden nodes and linear activation in the output nodes, it seems a reliable modus operandi.

\subsection{Learning algorithms}

NNs may be trained in various ways. Generally, learning algorithms - also called training algorithms - iteratively minimize an error function to adjust the weights on the nodes \citep{Nakamura.2005}. Backpropagation as the most widely used algorithm is based on the gradient-descent method \citep{Rojas.1996}, which is why differentiability of the activation functions and therefore of the error functions as well is necessary. After a random initialization of the network, the initial weights are adjusted according to the gradient of the error function. The Sum of Squared Errors (SSE) might be considered \citep{Rojas.1996}:
\begin{equation} E = \frac{1}{2}\sum_{i=1}^{p}||\hat{y}_{i} - y_{i}||^{2}\end{equation}

Another type of error function next to SSE may be Mean Squared Error (MSE) \citep{Zhang.1998}, which is defined by $ SSE/n$. Because the training set size is commonly much larger than the test set's, the training MSE will usually underestimate the test MSE \citep{James.2013}. \\
The derivatives of $E$, 
\begin{equation}
\nabla E(w) = {\frac{\partial E(w)}{\partial w_{1}}\frac{\partial E(w)}{\partial w_{2}},...,\frac{\partial E(w)}{\partial w_{l}}}
\end{equation}
 are minimized and the weights iteratively updated:

\begin{equation}
w_{i} = -\gamma\frac{\partial E(w)}{\partial w_{i}} for \textit{i} = 1, ...., \textit{l}
\end{equation}
, where $\gamma$ is the step size, also called learning rate. It determines the extent to which the weights are changed in each iteration and must carefully be specified, since too steep a descent might not converge and a too shallow one might be too slow \citep{Zhang.1998}.\\
In the end, $\nabla E$ is expected to be zero. 
When given input data, the network is fed forward by calculating the input to each node,
\begin{equation}
x_j = \sum_{i}y_{i}*w_{ji}
\end{equation}, where \textit{j} indexes output nodes, \textit{i} indexes the nodes connected to node \textit{j}, $w_{ji}$ denotes the weight and $y_{i}$ denotes the antecedent's node's output; outputs in turn are calculated by their activation functions. Furthermore, nodes can be given bias by adding an input with constant weight of 1\citep{Rumelhart.1986}.\\

Gradient-based learning algorithms can either be conducted in batches or on-line; in the former, all input-output patterns are processed parallely so that the weights are adjusted simultaneously; in the latter, the weights are updated sequentially \citep{Rojas.1996}.\\

The possible flaws of static learning rates have been tried to overcome by expanding the classic gradient descent method with a so-called momentum parameter that controls the direction of the change in weights to not diverge too far from the direction of the antecedent change in weight. (QUELLE)
\\
Backpropagation uses the first derivative of the error function is minimized; other algorithms use the Hessian matrix and thus the second derivative \citep{Rojas.1996}:
\begin{equation}
h = - (\nabla^2E(w))^{-1}\nabla E(w)
\end{equation}
where $w_{i}$ is then updated in the following way:
\begin{equation}
	w_{i} = w_{i-1} + h
\end{equation}

As it is an iterative algorithm and the Hessian matrix has to be computed in each iteration, Quasi-Newton methods make use of a simplification where only the diagonal elements of the Hessian matrix are used:
\begin{equation}
	w_{i} = w_{i-1} + \frac{\nabla_{i}E(w)}{\partial^{2}E(w)/\partial w_{i}^{2}}
\end{equation}
This method is based on the assumption of a quadratic error term.
\cite{Pinto.2005} use the Levenberg-Marquardt algorithm, a quasi-Newtonian learning algorithm, to improve learning speed and because it works especially well when there are only few observations; so do \cite{Benkachcha.2015}, \cite{Kourentzes.2014} and \cite{Zhou.2006}. \cite{Zhang.1998} report "faster convergence, robustness and the ability to find good local minima" as key advantages of second-order methods.\\

As already mentioned, NNs are very sensitive to the data they are trained on; thus, overfitting might occur: the model fits the training data very accurately but performs poorly in predicting new (test) data \citep{Nakamura.2005}. In order to avoid this issue, so-called early stopping of the training algorithm can be applied, i.e.training is stopped as soon as the training error term (e.g. MSE) starts increasing on a pre-selected validation data set.
The flip side of the coin is the problem of NNs being stuck on local minima - independently of the choice of learning algorithm \citep{Zhang.1998} - and therefore being biased; a method of circumvention is the running of multiple epochs, where all steps of training are passed with random initializations of parameters for each epoch. It is avisable to employ a \textit{burn-in} 

A third problem that may arise is the \textit{Vanishing Gradient Problem} \citep{Schmidhuber.2015}, which a network is more gravely affected by the deeper its architecture. With activation functions that can only take values between 0 and 1, and the cumulative backpropagated error terms that are computed by the chain rule, gradients become "vanishingly" small, as they are multiplied as many times as the network has layers, causing the weights to basically stop changing. If the activation function can take values larger than 1, the gradients may in contrast explode.\\
As a solution to \textit{Vanishing Gradient Problem}, the Long Short-Term Memory proposed by \cite{Hochreiter.1997}, which will be introduced further below.

\subsection{Network architecture}

\subsubsection{Feed-forward NNs}



In feed-forward MLP NNs, an input is fed to the input layer and traversed through the hidden layers without cycles until it reaches the output layer \citep{Rojas.1996}. Usually, alls nodes of one layer are connected to all nodes of the next layer, as shown in Picture NUMMER (Rojas p. 134)
Picture NUMMER (Benkachcha) shows an MLP feed-forward NN for time series forecasting where $Y_{t+1}$ is predicted by an input of its past values.\\


\subsubsection{Recurrent NNs}

According to \cite{Balkin.1997}, RNNs are better suited for time series data than FNNS; in fact, bad results in an RNN may hint that there is not time dependency in the data at all. 
A feedforward NN in its simplest form reduces to an $AR[p]$-model whereas an RNN may reduce to an $ARMA$-model, which is to be kept in mind when analyzing the data structure at hand.\\
In RNNs, additional "context units" are added to the model, next to the input layer. These store information from previous training loops. This way, they provide feedback about the past. Context units can either connect (1) the input layer with itself, (2) hidden layers with the input layer or (3 )the output layer with the input layer. (2) are called \textit{Elman networks}, whereas (3) are called \textit{Jordan networks}.
\cite{Balkin.1997} provides two Pictures illustrating the schemes of these two \textit{Simple Recurrent Networks}:
BILDER AUS BALKIN NUMMER

Long Shert-Term Memories (LSTM, \cite{Hochreiter.1997}) have evolved to become one of the most popular implementations of RNNs \citep{George.2017,Salehinejad.12292017}, as they can tackle the problem of \textit{vanishing gradients}. In LSTM, hidden units are modified to form \textit{memory cells} that have gates that control (1) input - how much new information is used - and (2) output  - or (3) the extent to which information from previous outputs is stored or "forgot". The self-connected \textit{Constant Error Carousels} within the memory cell block use the identity function as their activation function with weight 1; thus its derivative is always 1 so the gradients cannot vanish or explode \citep{Schmidhuber.2015}.
FIGURE shows an LSTM cell block.
\begin{align}
g^{i}_{t} = \sigma(W_{Ig^{i}}x_{t} + W_{Hg^{i}}h_{t-1}+W_{g^{c}g^{i}}g^{c}_{t-1} + b_{g^{i}}) \\
g^{f}_{t} = \sigma(W_{Ig^{f}}x_{t} + W_{Hg^{f}}h_{t-1}+W_{g^{c}g^{f}}g^{c}_{t-1} + b_{g^{f}}) \\
g^{o}_{t} = \sigma(W_{Ig^{o}}x_{t} + W_{Hg^{o}}h_{t-1}+W_{g^{c}g^{o}}g^{c}_{t-1} + b_{g^{o}}) 
\end{align}
where the matrices $W_{Ig^{\bullet}}$ is the weight matrix form the input layer, $W_{Hg^{\bullet}}$ is the weight matrix from the hidden layer and $+W_{g^{c}g^{\bullet}}$ is the weight matrix from the cell activation to the input, forget and output gate respectively, and $b_{g^{\bullet}}$ is the bias \citep{Salehinejad.12292017}.



%\begin{table}[]
%	\resizebox{\textwidth}{!}{
%		\begin{tabular}{llll}
%			& \textbf{ARIMA} & \textbf{SVR} & \textbf{ANN} \\
%			\textbf{Pro} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Efficient (Adhikari 2014)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Models linear\\   as well as nonlinear relationships (Han 2014,\\   Adhikari 2014)\\   ¬∑       \\   Only few data required (Wu 2008)\\   ¬∑       \\   Variable selection\\   ¬∑       \\   Able to find global maximum (Han 2014)\\   ¬∑       \\   strong generalization ability  (Adhikari 2014) \\   ¬∑       \\   faster convergence speed (Wu 2008)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   models nonlinear relationships\\   ¬∑       \\   variable selection\\   ¬∑       \\   useful for ‚Äûlong-term‚Äú time series \\   (Han 2014)\\   ¬∑       \\   non-parametric and nonlinear modeling skill, flexible and data-driven nature,\\   good generalizationability, and reasonably good accuracy. (Adhikari 2014)\\   ¬∑       \\   RNN capable of including time dependencies (Adhikari 2014)\end{tabular} \\
%			\textbf{Con} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Models linear relationships between variables ‚Äì accuracy my be\\   afflicted (Adhikari 2014)\\   ¬∑       \\   Easy to be disturbed by noise (Wu 2008)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   Training times increase\\   drastically with respect to the number of training samples (Deng 2005)\end{tabular} & \begin{tabular}[c]{@{}l@{}}¬∑       \\   May lead to overfitting (Deng 2005)\\   ¬∑       \\   May only find local maximum (Deng 2005, Adhikari 2014)\\   ¬∑       \\   Sensitive for preselected model architecture and data fed to the model\\   (Adhikari 2014)\\   ¬∑       \\   Computational cost RNN \textgreater FNN (Adhikari 2014)\end{tabular} \\
%			&  &  & 
%		\end{tabular}
%	}
%\end{table}

\section{Variable selection}

A well-defined model should include all relevant explanatory variables that contribute to the variance found in the target value and exclude irrelevant, noisy variables. This way, the data and the process constituting it can be better understood, computational time and the so-called curse of dimensionality can be reduced while maintaining a high prediction accuracy \citep{Chandrashekar.2014}. The curse of dimensionality describes the problem arising in a high dimensional (i.e. containing many variables) space, where one needs very large samples to accurately depict said space (QUELLE). Variable selection is furthermore important as irrelevant data may cause poor generalization when given new data \citep{Chandrashekar.2014, BenIshak.2016}.\
Variable selection has become an important issue as research is increasingly focussing on high dimensional data, be it in the field of genetics or Data Mining \citep{BenIshak.2016}. Fortunately, in this case $n > p$, which is why there is no need to give up on reasonable generalization ability, computational speed or model interpretability in favour of a heavily reduced variable subset.\

As with many questions that arise in the process of analyzing data, the question of how to best select input variables in a time series context, and in this case, to predict freight weight, cannot be answered easily. \ 
Directly comparing all $2^p$ possible subsets of variables is computationally unfeasible, so it is most often infeasible to compare all possible variable subsets and choose the best one. 
\\
\cite{Wit.2012} argue that there is not variable that has zero contribution to the target, and therefore - philiosophically - model selection is a comparison between different sets of patterns (i.e. sets of variables) in order to choose which pattern to focus on, depending on the context in which a research question is proposed. Variable selection also takes great part in the variance-bias tradoff, or whether to seek a near-perfect  but perhaps overly complex fit of past data or a sparse, economic but perhaps overly simple model. Furthermore, minimal computation time and and a reasonably good prediction accuracy are enhanced by thorough variable selection \citep{Saeys.2007}.  Obviously, the aim should be to find a golden mean between sparsity and accuracy, too many and too few variables. 

A first selection of variables should be given by experts, literature of the same field or common sense \citep{Heinze.2018}. This is an important step as is may enhance model performance and facilitate the choice of a suitable subset of variables \citep{BenIshak.2016}
Next to the researchers' expertise, there exist several variable selection methods that quantify the explanatory value of each possible variable and include or exclude them in the final model by a prespecified selection criterion.\


Albeit the growing number of available methods for variable selection, neither is there one gold standard that is generally useful, nor is there one method especially advisable for each kind of data \citep{Taghizadeh.2017}\\

\subsection{Feature Selection Methods in Machine Learning}
Feature selection methods, as variable selection methods are called in the context of Machine Learning, can be classified into \textit{Filter}, \textit{Wrapper} and \textit{Embedded methods} \citep{Chandrashekar.2014}, . \
\subsubsection{Filter Methods}
\textit{Filter Methods} are mainly used in text classification and include standard statistical measures such as Euclidian distance, the Chi-Squared test, the t-Test, ANOVA, correlation coefficients, Markov Blankets
and the Fisher Score. 
Adavantages of Filter Methods are their fast computation times, independency of the classification methods - they only make use of intrinsic properties of the variables - and that feature selection only has to be conducted once \citep{Saeys.2007}.\
But, as they test variables seperately "against" the target variable (e.g. for correlation) to rank them, there's a risk that redundant variables are taken into the model when chosen by a Filter Method \citep{Chandrashekar.2014}. Consequently, it is possible that the model established by a Filter Method is not unique, as a different subsets of variables with similar properties w.r.t the Filter Method might have yielded an equal result.\

\subsubsection{Wrapper Methods}
\textit{Wrapper Methods} measure the predictors' performance through a search algorithm. These methods include \textit{Forward} and \textit{Backward Selection} as well as \textit{Heuristic search algorithms} such as \textit{Simulated Annealing} and \textit{Genetic Algorithms}.\
Advantageously, \textit{Wrapper Methods} take into account the interdependence of variables, but unfortunately require high computation times and are prone to overfitting \citep{Saeys.2007}.\


\subsubsection{Embedded Methods}              
Both \textit{Wrapper} and \textit{Embedded Methods} are closely intertwined with the classification algorithm \citep{Saeys.2007}
In \textit{Embedded Methods}, variable subset selection is part of the model's training \citep{Chandrashekar.2014}, and they combine the search for a variable subset and hypothesis testing \citep{Saeys.2007}. They are better able to reduce computation time than Wrapper Methods. An example for an Embedded Method are CARTs (Classification And Regression Trees).
\\

\subsection{Variable Selection in AR(I)MA, SVR and NNs}
\cite{Chandrashekar.2014} mentions Support Vector Machines and Neural Networks as part of \textit{Embedded Methods}. Resulting from this, for these two methods, variable selection (except for a literature- and expert-driven pre-selection) and model fit could be combined in one step. 
Because the AR(I)MAX model can be specified as a linear regression with autocorrelated errors, it is possible to use a stepwise variable selection comparing AICs of the different specifications. Afterwards, the contribution of the selected variable to the forecast can be compared to the baseline AR(I)MA model using only the target variable. %As for AR(I)MA, \textit{Forward} or \textit{Backward Selection} techniques using ICs such as AIC or BIC are favourable because ????. \\

Other approaches to variable selection in the context of freight demand and / or time series forecasting have been found:
In their study on the influence of national, regional and local economic indices on freight volume for different segments of the truckload industry in the USA, \cite{Fite.2002} first examined correlation matrices before using stepwise multiple regression to draw a selection from the large set of initial variables collected. 
\cite{Taghizadeh.2017} suggest using ensemble methods such as Random Forest to take advantage of several different machine learning methods.
\cite{Feng.2018} used Spearman rank correlation to select external variables to predict railway freight volume.
Variable selection methods for time series data have also been explored, using NNs (QUELLE), SVMs (QUELLE) and other, Filter-like techniques (QUELLE).\\

Obviously, numerous approaches exist that use different methods and comprise of Filter, Wrapper or Embedded methods. \
In this coursework, to ensure comparability between methods, the results of the AR(I)MA and AR(I)MAX modelling process could  be used as the baseline models for SVR and NN. This way, the performance of the AR(I)MA, SVR and NN can be directly compared without being confounded with the use of different subsets of variables for each method. \
Having said that, the goal of this coursework is to compare the \textit{performance} of different methods as well as models using additional explanatory variables versus the baseline time series. As AR(I)MA assumes a linear relationship between variables, using this approach to create a subset of additional variables might hamper the performance of SVR and NN. SVR and NN are able to map nonlinear relationships, but being given a subset of variables that was found using a linear assumption might turn out counterproductive. \
One could consolidate these two arguments by tayloring a variable selection process for each inidivdual method and then compare the subsets retrieved by the AR(I)MA, SVR and NN models respectively. Variables included in each subset are chosen for the final model, whereas others. One way to ensure some kind of equal conditions could be to use a \textit{Wrapper Method} like e.g. stepwise selection for each method; the procedure would be the same, but adapted to ("wrapped around") the method.


\section{Model Evaluation}

Generally, data sets are partitioned into training, validation and test data sets in order to estimate parameters (with the training set), fine-tune them during training (on the validation set) and evaluate the model (on the test set). While a validation set is not needed for ARIMA models, it is necessary for NNs \citep{Zhou.2006}. \
Although usually the partitioning is random (in non-time-series data at least) to ensure a good variance of values in the partial data sets, the question arises whether a good (or bad) model performance on the test set is due to the model being well-specified or whether it is not - at least in part - due to the way the data was split. To help eliminate doubts on the accuracy of the model evaluation,  \textit{cross-validation} is used.\ QUELLE
In \textit{k-fold cross-validation}, the data set is split into \textit{k} folds. The model is trained on $k-1$ folds and tested on the one retained fold, each fold in turn being the \textit{"test fold"} once. \
In time series, it does not make sense to randomly split the data set into groups, since then the model would also be trained on future values attempting to predict past values. Using the idea of cross-validation sensibly, one could partition the data set in such a ways that in the training set, only values preciding the values in the test set are found. \
A possible setting may look like the one suggested by \cite{Hyndman.2018}: The data is sorted by timestamps and then trained on the first $p$ values. Then, the model is tested on the single value $p+1$. Next, the model is trained on the first $p+1$ values and tested on the $p+2$'nd value and so on. This way, model evaluation makes use of all available observations and the results remain comparable since the test set size remains the same. Asymptotically, this technique, called the "rolling forecast origin", yields the same results as minimization of the \textit{AIC}. \\
Common metrics to evaluate models (and compare them) are the \textit{Root Mean Square Error (RMSE)} and the \textit{Mean Average Percentage Error (MAPE)}, the latter one of which is especially useful in time series models with a strong trend \citep{Moritz2015ComparisonOD}. 

\begin{align}
	RMSE = \sqrt{\frac{\sum_{t = 1}^{n}(\overline{y}_{t}-y_{t})^2}{n}}\\
	\newline
	MAPE = \sqrt{\frac{\sum_{t = 1}^{n}\frac{|\overline{y}_{t}-y_{t}|}{|y_{t}|}}{n}}\
\end{align}


\section{Software implementation}

Data preparation and analyses were conducted in \texttt{R} \citep{Team.2013}. The NN was constructed in Python 3.7.2 using Keras \citep{chollet2015keras} with Tensorflow \citep{tensorflow2015-whitepaper} backend. A comprehensive list of the packages used in R and in Python can be found in Appendix (NUMMER).\\

For all common functions, \texttt{stats} was used whenever possible. Furthermore, , \cite{Hyndman.2008}'s  \texttt{forecast} package was chosen for most functions concerning AR(I)MA modeling. If not otherwise specified, functions mentioned in the following belong to said package.

Missing values for time series were imputed by linear interpolation using the function \texttt{na.interp}, which was found to supply robust imputation compared to other functions for imputation of mmissing values in time series available in \texttt{R} \citep{Moritz2015ComparisonOD}. 

To specify the AR(I)MA model, the \texttt{arima()} function in \texttt{stats{}} was used.\
For ACF and PACF plots and the AR(I)MAX model, the functions \texttt{acf()}, \texttt{pacf()}, and \texttt{arima()} (which is essentially a wrapper on \texttt{stats}'s \texttt{arima()}, including some extensions, e.g. exogenous variables PR√úFEN). \texttt{Arima()} actually fits a linear regression with AR(I)MA errors \citep{Hyndman.2018}; no \texttt{R} function could be found that models actual ARIMAX models.\\
For ADF tests, the function \texttt {adf.test()} in the package \texttt{tseries} was used. In the function, the parameter \textit{k} (i.e. the lag) is determined according to the formula suggested by \cite{Vogel.2015} (see subsection NUMMER (Violation of assumptions)), although without t-testing the coefficients of equation NUMMER 4. Manually reducing \textit{k} and comparing the results yielded no differences in the outcome.\\

Subsequently, the parameters are computed bei \texttt{Arima()}; possible seasonal components can be modeled by the \texttt{fourier()} functions given as an argument to \texttt{xreg=} in \texttt{Arima()}. The order of the fourier series is determined by minimizing the AICc (AICC ERKL√ÑREN s oben). \
%As the \texttt{Arima()} function in the \texttt{forecast} package in \texttt{R} uses a linear regression with AR(I)MA errors, variable selection for the exogenous variables in the time series model was conducted using the \texttt{step()} function comparing the AIC in the models.\\

Functions to model Support Vector Machines and Regression are contained in the package \texttt{e1071}.\\

NN IN KERAS

RFE

The error metrics RMSE and MAPE were customly computed.\\





\subsection{Data analysis}

\subsubsection{Expert-based variable selection}

This thesis is part of a Fraunhofer institute's department for Supply Chain Services (Fraunhofer SCS)'s research project, in cooperation with the University of Erlangen-Nuremberg, the University of Erlangen and NAGEL group as one of several industrial partners. 
One goal of this research project is to elaborate whether the addition of further information is helpful to predict freight demand (in terms of weight or volume) as opposed to solely basing the forecast on past values.\\
As the research project is funded by the Federal Ministry for Traffic and Digital Infrastructure, a precondition to which kind of information to choose was to use freely available data sets, such as data provided by the Ministry's \textit{mcloud}, an open online platform with data on mobility and related topics. \\
When planning the research project, initial ideas were to explore different time scopes and check for seasonalities and calender-related influences on freight demand, such as weekly, monthly, quarterly, or yearly fluctuations and changes in demand due to (upcoming) holidays. \

In \cite{Abate.2014}'s study on the effect of fuel prices on average length of haul, inconsistent results were found: A decrease in length of haul before 2008 and an increase after. Furthermore, an improvement of load capacity was detected for higher fuel prices, but also a decline in physical productivity, measured in tonnekilometres.
Due to the inconsistency of the findings in this, it is unclear whether fuel price might have an impact on freight weight. Logical considerations, on the one hand, could be that higher prices would hamper demand in deliveries by truck and likely cause a shift towards e.g. rail transportation. On the other hand, infrastructure might not always allow a shift towards other means of transportation, and certain (basic) types of food are demanded regardless of price, so there might not be an influence of fuel price on freight weight (in general), or only on specific types of food, e.g. luxury goods.
Data on fuel prices are accessible from the website of the "German General Automobile Club" (Allgemeiner Deutscher Automobilclub, ADAC) \citep{ADAC.}.\

\cite{Cools.2009} accounted for holiday and daily effects on traffic counts and found weekly patterns and holiday impacts especially on the amount of commuters. Since their target was the quantity of passenger transportation on highways and not goods traffic, their results do not exactly indicate the inclusion of holiday and daily effects into the model in this coursework. However, as experts of the industrial partners of Fraunhofer SCS stated the importance of these two factors in their past experience, they are going to be included in this coursework. \cite{Cools.2009}'s paper provides a useful guidance to how to handle these two variables in time series forecasting.\
Data on holidays and school holidays can be retrieved from the website "schulferien.org" \citep{.b}.\\

According to \cite{JONG.2004}, GDP has been included as an explanatory variable in other freight models before; \cite{Intihar} included GDP in their paper as well, and furthermore unemployment. Both GDP (in German: BIP) and unemployment rate are data accessible from the German Federal Statistical Office (Destatis)'s database "GENESIS" \citep{StatistischesBundesamt.2019}.\


All data retrieved were restricted to the Federal State of Bavaria between 2015 and 2017.
\bibliographystyle{apalike}
\bibliography{jusmabib}
\end{document} 